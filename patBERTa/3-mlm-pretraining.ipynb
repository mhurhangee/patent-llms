{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad571f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21104' max='21104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21104/21104 55:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2638</td>\n",
       "      <td>5.478000</td>\n",
       "      <td>5.027371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5276</td>\n",
       "      <td>3.848200</td>\n",
       "      <td>3.417055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7914</td>\n",
       "      <td>3.362500</td>\n",
       "      <td>2.959890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10552</td>\n",
       "      <td>3.088700</td>\n",
       "      <td>2.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13190</td>\n",
       "      <td>2.983500</td>\n",
       "      <td>2.651377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15828</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>2.563684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18466</td>\n",
       "      <td>2.820700</td>\n",
       "      <td>2.503295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21104</td>\n",
       "      <td>2.782800</td>\n",
       "      <td>2.484285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.decoder.weight', 'lm_head.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='431' max='431' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [431/431 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4812066555023193, 'eval_runtime': 6.727, 'eval_samples_per_second': 512.116, 'eval_steps_per_second': 64.07, 'epoch': 2.0} Perplexity: 11.95568210710622\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# patRoBERTa Toy MLM — Simple & Stable\n",
    "# =========================================\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    RobertaConfig, RobertaForMaskedLM, RobertaTokenizerFast,\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "# ---------- CONSTANTS (edit) ----------\n",
    "TRAIN_TXT     = \"../data/ep-b1-claim1-corpus/ep-b1-claim1-cpc_train.txt\"\n",
    "VAL_TXT       = \"../data/ep-b1-claim1-corpus/ep-b1-claim1-cpc_val.txt\"\n",
    "ENCODINGS_DIR = Path(\"../data/patroberta-encoded-128-vs8000\")\n",
    "TOKENIZER_DIR = \"../artifacts/patroberta-tokenizers/vs8000\"\n",
    "\n",
    "SEQ_LEN = 128                  # keep small on 4GB\n",
    "ARCH_MAX_POSITIONS = 514       # capacity (OK for 512 later)\n",
    "MLM_PROB = 0.15\n",
    "\n",
    "# Tiny model\n",
    "HIDDEN_SIZE, NUM_LAYERS, NUM_HEADS, INTER_SIZE = 128, 2, 2, 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training knobs (keep tiny to avoid OOM)\n",
    "PER_DEVICE_TRAIN_BS = 16       # fits 4GB with fp16 + checkpointing\n",
    "PER_DEVICE_EVAL_BS  = 8\n",
    "GRAD_ACCUM_STEPS    = 2        # effective batch = 32 sequences\n",
    "LEARNING_RATE       = 5e-4\n",
    "WEIGHT_DECAY        = 0.01\n",
    "WARMUP_RATIO        = 0.06\n",
    "FP16                = True\n",
    "NUM_EPOCHS          = 2        # or set MAX_STEPS instead of epochs\n",
    "\n",
    "OUT_DIR = \"../artifacts/patroberta-mlm-128-simple\"\n",
    "\n",
    "# ---------- Tokenizer & data ----------\n",
    "tok = RobertaTokenizerFast.from_pretrained(TOKENIZER_DIR)\n",
    "tok.model_max_length = SEQ_LEN\n",
    "\n",
    "def enc(b): return tok(b[\"text\"], truncation=True, max_length=SEQ_LEN)\n",
    "\n",
    "if ENCODINGS_DIR.exists():\n",
    "    ds = load_from_disk(str(ENCODINGS_DIR))\n",
    "else:\n",
    "    raw = load_dataset(\"text\", data_files={\"train\": TRAIN_TXT, \"validation\": VAL_TXT})\n",
    "    ds = raw.map(enc, batched=True, remove_columns=[\"text\"])\n",
    "    ds.save_to_disk(str(ENCODINGS_DIR))\n",
    "\n",
    "# ---------- Collator ----------\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=True, mlm_probability=MLM_PROB)\n",
    "\n",
    "# ---------- Model ----------\n",
    "cfg = RobertaConfig(\n",
    "    vocab_size=tok.vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_hidden_layers=NUM_LAYERS,\n",
    "    num_attention_heads=NUM_HEADS,\n",
    "    intermediate_size=INTER_SIZE,\n",
    "    hidden_dropout_prob=DROPOUT,\n",
    "    attention_probs_dropout_prob=DROPOUT,\n",
    "    max_position_embeddings=ARCH_MAX_POSITIONS,\n",
    "    pad_token_id=tok.pad_token_id,\n",
    "    bos_token_id=tok.bos_token_id,\n",
    "    eos_token_id=tok.eos_token_id,\n",
    ")\n",
    "model = RobertaForMaskedLM(cfg)\n",
    "model.gradient_checkpointing_enable()  # big saver on 4GB\n",
    "\n",
    "# ---------- Auto-scale logging by epoch fraction ----------\n",
    "steps_per_epoch = max(1, math.ceil(len(ds[\"train\"]) / (PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS)))\n",
    "EVAL_STEPS     = max(1, steps_per_epoch // 4)   # 4× per epoch\n",
    "SAVE_STEPS     = EVAL_STEPS\n",
    "LOGGING_STEPS  = max(1, steps_per_epoch // 10)  # 10× per epoch\n",
    "\n",
    "# ---------- Training args ----------\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    fp16=FP16,\n",
    "    fp16_full_eval=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    torch_empty_cache_steps=LOGGING_STEPS,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    processing_class=tok,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "out = trainer.evaluate()\n",
    "import math as _m\n",
    "print(out, \"Perplexity:\", _m.exp(out[\"eval_loss\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd8d69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37636' max='135388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 37636/135388 6:01:12 < 15:38:11, 1.74 it/s, Epoch 1.11/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3384</td>\n",
       "      <td>5.435700</td>\n",
       "      <td>4.855713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6768</td>\n",
       "      <td>3.639600</td>\n",
       "      <td>3.119433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10152</td>\n",
       "      <td>2.992600</td>\n",
       "      <td>2.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13536</td>\n",
       "      <td>2.711200</td>\n",
       "      <td>2.381478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16920</td>\n",
       "      <td>2.574700</td>\n",
       "      <td>2.265131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20304</td>\n",
       "      <td>2.492100</td>\n",
       "      <td>2.197931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23688</td>\n",
       "      <td>2.432600</td>\n",
       "      <td>2.140914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27072</td>\n",
       "      <td>2.392400</td>\n",
       "      <td>2.105449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30456</td>\n",
       "      <td>2.356900</td>\n",
       "      <td>2.083310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33840</td>\n",
       "      <td>2.318400</td>\n",
       "      <td>2.042159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37224</td>\n",
       "      <td>2.299700</td>\n",
       "      <td>2.024109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m     83\u001b[39m args = TrainingArguments(\n\u001b[32m     84\u001b[39m     output_dir=OUT_DIR,\n\u001b[32m     85\u001b[39m     per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m     torch_empty_cache_steps=LOGGING_STEPS,\n\u001b[32m    104\u001b[39m )\n\u001b[32m    106\u001b[39m trainer = Trainer(\n\u001b[32m    107\u001b[39m     model=model,\n\u001b[32m    108\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m                                          early_stopping_threshold=THRESH)],\n\u001b[32m    115\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m out = trainer.evaluate()\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_m\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/patent-llms/.venv/lib/python3.11/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/patent-llms/.venv/lib/python3.11/site-packages/transformers/trainer.py:2587\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2582\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2591\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# patRoBERTa Toy MLM — Simple & Stable\n",
    "# =========================================\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    RobertaConfig, RobertaForMaskedLM, RobertaTokenizerFast,\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# ---------- CONSTANTS (edit) ----------\n",
    "TRAIN_TXT     = \"../data/1-corpus-all-claims/corpus_train.txt\"\n",
    "VAL_TXT       = \"../data/1-corpus-all-claims/corpus_val.txt\"\n",
    "ENCODINGS_DIR = Path(\"../data/3-encodings/sl128-v8000\") # Will create if encoding doesn't exist\n",
    "TOKENIZER_DIR = \"../data/2-tokenizers/vs8000\"\n",
    "\n",
    "SEQ_LEN = 128                  # keep small on 4GB\n",
    "ARCH_MAX_POSITIONS = 514       # capacity (OK for 512 later)\n",
    "MLM_PROB = 0.15\n",
    "\n",
    "# Tiny model\n",
    "HIDDEN_SIZE, NUM_LAYERS, NUM_HEADS, INTER_SIZE = 128, 2, 2, 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training knobs\n",
    "PER_DEVICE_TRAIN_BS = 64 \n",
    "PER_DEVICE_EVAL_BS  = 8\n",
    "GRAD_ACCUM_STEPS    = 2        # effective batch = 32 sequences\n",
    "LEARNING_RATE       = 5e-4\n",
    "WEIGHT_DECAY        = 0.01\n",
    "WARMUP_RATIO        = 0.06\n",
    "FP16                = True\n",
    "NUM_EPOCHS          = 4\n",
    "\n",
    "# Early stopping\n",
    "PATIENCE = 3                     # evals with no improvement\n",
    "THRESH   = 1e-4                  # min improvement in eval_loss\n",
    "\n",
    "OUT_DIR = \"../data/3-pretaining/mlm-sl128-v8000\"\n",
    "\n",
    "# ---------- Tokenizer & data ----------\n",
    "tok = RobertaTokenizerFast.from_pretrained(TOKENIZER_DIR)\n",
    "tok.model_max_length = SEQ_LEN\n",
    "\n",
    "def enc(b): return tok(b[\"text\"], truncation=True, max_length=SEQ_LEN)\n",
    "\n",
    "if ENCODINGS_DIR.exists():\n",
    "    ds = load_from_disk(str(ENCODINGS_DIR))\n",
    "else:\n",
    "    raw = load_dataset(\"text\", data_files={\"train\": TRAIN_TXT, \"validation\": VAL_TXT})\n",
    "    ds = raw.map(enc, batched=True, remove_columns=[\"text\"])\n",
    "    ds.save_to_disk(str(ENCODINGS_DIR))\n",
    "\n",
    "# ---------- Collator ----------\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=True, mlm_probability=MLM_PROB)\n",
    "\n",
    "# ---------- Model ----------\n",
    "cfg = RobertaConfig(\n",
    "    vocab_size=tok.vocab_size,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_hidden_layers=NUM_LAYERS,\n",
    "    num_attention_heads=NUM_HEADS,\n",
    "    intermediate_size=INTER_SIZE,\n",
    "    hidden_dropout_prob=DROPOUT,\n",
    "    attention_probs_dropout_prob=DROPOUT,\n",
    "    max_position_embeddings=ARCH_MAX_POSITIONS,\n",
    "    pad_token_id=tok.pad_token_id,\n",
    "    bos_token_id=tok.bos_token_id,\n",
    "    eos_token_id=tok.eos_token_id,\n",
    ")\n",
    "model = RobertaForMaskedLM(cfg)\n",
    "model.gradient_checkpointing_enable()  # big saver on 4GB\n",
    "\n",
    "# ---------- Auto-scale logging by epoch fraction ----------\n",
    "steps_per_epoch = max(1, math.ceil(len(ds[\"train\"]) / (PER_DEVICE_TRAIN_BS * GRAD_ACCUM_STEPS)))\n",
    "EVAL_STEPS     = max(1, steps_per_epoch // 10)   # 4× per epoch\n",
    "SAVE_STEPS     = EVAL_STEPS\n",
    "LOGGING_STEPS  = max(1, steps_per_epoch // 20)  # 10× per epoch\n",
    "\n",
    "# ---------- Training args ----------\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
    "    per_device_eval_batch_size=PER_DEVICE_EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=4,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    fp16=FP16,\n",
    "    fp16_full_eval=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    torch_empty_cache_steps=LOGGING_STEPS,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    processing_class=tok,\n",
    "    data_collator=collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE,\n",
    "                                         early_stopping_threshold=THRESH)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "out = trainer.evaluate()\n",
    "import math as _m\n",
    "print(out, \"Perplexity:\", _m.exp(out[\"eval_loss\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3ae36",
   "metadata": {},
   "source": [
    "# Push model to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26374782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3657b56a7a5a49f6ba7cd4e67a685eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424514a04ba840259a836b02460b3489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b507640e5df40e78eda6455a0d99355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmp5nrkud6u/model.safetensors    :   9%|9         |  554kB / 6.05MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc66147ba624a2ab34f235ba6eeae81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mhurhangee/patroberta-mlm-sl128-v8000/commit/d3ea4e75a17a328f5b9135cbfb594fd67be1af5b', commit_message='Upload tokenizer', commit_description='', oid='d3ea4e75a17a328f5b9135cbfb594fd67be1af5b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mhurhangee/patroberta-mlm-sl128-v8000', endpoint='https://huggingface.co', repo_type='model', repo_id='mhurhangee/patroberta-mlm-sl128-v8000'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM, RobertaTokenizerFast\n",
    "\n",
    "model_dir = \"../data/3-pretaining/mlm-sl128-v8000/checkpoint-33840\" # 1 epoch\n",
    "repo_name = \"mhurhangee/patroberta-mlm-sl128-v8000\"\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained(model_dir)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_dir)\n",
    "\n",
    "model.push_to_hub(repo_name)\n",
    "tokenizer.push_to_hub(repo_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ade67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push whole checkpoint for resuming to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f8fb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fb624606d84295835ada1a8720e1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efaa7bd984d442688f3745e12061fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456dc59342694f1faea6ad0a89c89002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  .../checkpoint-33840/model.safetensors: 100%|##########| 6.05MB / 6.05MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063781357a0e4b069bd4f84ade429c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...v8000/checkpoint-33840/optimizer.pt:   5%|4         |  588kB / 12.1MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78dbc9d14d14b26b042a18198ac1369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...8000/checkpoint-33840/rng_state.pth:   5%|4         |   710B / 14.6kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3489a3b7a6fd484aab00821e11a2a6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...28-v8000/checkpoint-33840/scaler.pt:   5%|4         |  67.0B / 1.38kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb05237fe86e4ec2b27d6a4d4e145f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...v8000/checkpoint-33840/scheduler.pt:   5%|4         |  71.0B / 1.47kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f8505cc9054ca4bbe1543863bf3d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  .../checkpoint-33840/training_args.bin:   5%|4         |   280B / 5.78kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mhurhangee/patroberta-mlm-sl128-v8000/commit/08a406030cbf8b53c8d52bbf99634dbdbe96b8ab', commit_message='Upload full Trainer checkpoint for resume', commit_description='', oid='08a406030cbf8b53c8d52bbf99634dbdbe96b8ab', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mhurhangee/patroberta-mlm-sl128-v8000', endpoint='https://huggingface.co', repo_type='model', repo_id='mhurhangee/patroberta-mlm-sl128-v8000'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "\n",
    "# Path to your Trainer checkpoint directory\n",
    "checkpoint_dir = \"../data/3-pretaining/mlm-sl128-v8000/checkpoint-33840\"\n",
    "repo_name = \"mhurhangee/patroberta-mlm-sl128-v8000\"\n",
    "\n",
    "upload_folder(\n",
    "    folder_path=checkpoint_dir,\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Upload full Trainer checkpoint for resume\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
