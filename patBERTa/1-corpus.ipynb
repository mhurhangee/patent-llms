{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526aad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split ratios → train 98%, val 1%, test 1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae0468ab43047c399965cce478d22b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/204 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stats ===\n",
      "Files processed           : 204\n",
      "Raw claims seen           : 394242\n",
      "Kept (after cleaning/dedupe): 344539\n",
      "Dropped (length)          : 49504\n",
      "Dropped (charset)         : 0\n",
      "Duplicates removed        : 199\n",
      "\n",
      "=== Outputs ===\n",
      "Train lines: 337648 → ../data/1-corpus-ind-claims_train.txt\n",
      "Val lines  : 3445   → ../data/1-corpus-ind-claims_val.txt\n",
      "Test lines : 3446  → ../data/1-corpus-ind-claims_test.txt\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# patnroberta - Independent Claims Corpus Builder\n",
    "# ==============================\n",
    "# - Processes EPO JSONL files containing claims\n",
    "# - Cleans text, normalizes reference numerals, dedupes\n",
    "# - Produces train/val/test line-separated corpora with adjustable ratios\n",
    "# ==============================\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, unicodedata, string, random, math\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Iterable, Tuple, Dict, List\n",
    "\n",
    "# ---------- Config ----------\n",
    "INPUT_FOLDER = \"../data/0-scraped\"\n",
    "OUTPUT_PREFIX = \"../data/1-corpus-ind-claims/corpus\"   # will write *_train.txt, *_val.txt, *_test.txt\n",
    "\n",
    "# Cleaning/normalization\n",
    "REFS_MODE    = \"replace\"     # \"keep\" | \"remove\" | \"replace\"  (replace -> <REFNUM>)\n",
    "ADD_EOS      = False         # RoBERTa does not need EOS in raw text\n",
    "ALL_CLAIMS   = False         # True = keep all claims; False = only independent claim (first)\n",
    "NFKC         = True          # normalize to NFKC\n",
    "DEDUPE       = True          # deduplicate lines (normalized key)\n",
    "MIN_LEN      = 20            # 0 = disable\n",
    "MAX_LEN      = 5000          # 0 = disable\n",
    "\n",
    "# Charset guards (byte-BPE friendly): do not penalize non-ASCII\n",
    "MIN_PRINTABLE_RATIO = 0.98   # drop lines with < this fraction of printable chars\n",
    "MAX_NONASCII_RATIO  = 0.0    # 0 disables the non-ASCII cap\n",
    "\n",
    "# Splits (must sum to 1.0). Examples: (0.98, 0.01, 0.01) or (0.90, 0.05, 0.05)\n",
    "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.98, 0.01, 0.01\n",
    "RANDOM_SEED = 42             # for deterministic shuffling\n",
    "\n",
    "# ---------- Patterns ----------\n",
    "# Reference numerals like (101), [0032], {12, 14}, etc.\n",
    "REF_PARENS = r\"\"\"[\\(\\[\\{]\\s*(?:\\d+[A-Za-z]*[′'″]*)(?:\\s*,\\s*\\d+[A-Za-z]*[′'″]*)*\\s*[\\)\\]\\}]\"\"\"\n",
    "REF_REGEX  = re.compile(REF_PARENS)\n",
    "\n",
    "# whitespace collapse\n",
    "WS = re.compile(r\"\\s+\")\n",
    "\n",
    "# digits collapse (for dedupe key)\n",
    "DIGITS = re.compile(r\"\\d+\")\n",
    "\n",
    "# Allow extra useful Unicode in patents\n",
    "PRINTABLE_SET = set(string.printable) | {\"’\",\"“\",\"”\",\"–\",\"—\",\"·\",\"•\",\"°\",\"µ\",\"²\",\"³\",\"±\",\"≥\",\"≤\",\"·\",\"½\",\"¼\",\"¾\",\"™\",\"®\",\"§\"}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_line_charset_ok(s: str) -> bool:\n",
    "    if not s:\n",
    "        return False\n",
    "    total = len(s)\n",
    "    printable = sum((ch in PRINTABLE_SET) or ch.isprintable() for ch in s)\n",
    "    if total == 0 or printable / total < MIN_PRINTABLE_RATIO:\n",
    "        return False\n",
    "    if MAX_NONASCII_RATIO:\n",
    "        nonascii = sum(ord(ch) > 127 for ch in s)\n",
    "        if (nonascii / total) > MAX_NONASCII_RATIO:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def process_ref_numerals(text: str, mode: str) -> str:\n",
    "    if mode == \"keep\":\n",
    "        return text\n",
    "    if mode == \"remove\":\n",
    "        return REF_REGEX.sub(\" \", text)\n",
    "    if mode == \"replace\":\n",
    "        out = REF_REGEX.sub(\" <REFNUM> \", text)\n",
    "        # collapse repeats of <REFNUM>\n",
    "        return re.sub(r\"(?:\\s*<REFNUM>\\s*){2,}\", \" <REFNUM> \", out)\n",
    "    raise ValueError(\"REFS_MODE must be 'keep', 'remove', or 'replace'\")\n",
    "\n",
    "def clean_claim(text: str, refs_mode: str, add_eos: bool, nfkc: bool) -> str:\n",
    "    t = text.strip().replace(\"\\n\", \" \")\n",
    "    if nfkc:\n",
    "        t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = process_ref_numerals(t, refs_mode)\n",
    "    t = WS.sub(\" \", t).strip()\n",
    "    if add_eos and not t.endswith(\"<EOS>\"):\n",
    "        t = f\"{t} <EOS>\"\n",
    "    return t\n",
    "\n",
    "def dedupe_key(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a normalized key for deduplication while keeping original text cased.\n",
    "    - remove <REFNUM>\n",
    "    - collapse digits to 0\n",
    "    - normalize spaces\n",
    "    - lowercase for key only\n",
    "    \"\"\"\n",
    "    k = t.replace(\"<REFNUM>\", \" \")\n",
    "    k = DIGITS.sub(\"0\", k)\n",
    "    k = \" \".join(k.split()).lower()\n",
    "    return k\n",
    "\n",
    "def validate_splits(train_r: float, val_r: float, test_r: float) -> Tuple[int,int,int]:\n",
    "    s = train_r + val_r + test_r\n",
    "    if abs(s - 1.0) > 1e-8:\n",
    "        raise ValueError(f\"Split ratios must sum to 1.0, got {s}\")\n",
    "    # Return as percentages for logging\n",
    "    return int(train_r*100), int(val_r*100), int(test_r*100)\n",
    "\n",
    "# ---------- Main processing ----------\n",
    "def iter_claims_from_file(fp: Path, all_claims: bool) -> Iterable[str]:\n",
    "    with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        for line in fh:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            claims = data.get(\"c\", {})\n",
    "            if isinstance(claims, dict):\n",
    "                values = list(claims.values())\n",
    "            elif isinstance(claims, list):\n",
    "                values = claims\n",
    "            else:\n",
    "                values = []\n",
    "            for i, claim_text in enumerate(values):\n",
    "                if not isinstance(claim_text, str):\n",
    "                    continue\n",
    "                yield claim_text\n",
    "                if not all_claims:\n",
    "                    break  # only the first (independent) claim\n",
    "\n",
    "def build_corpus(in_dir: str) -> Tuple[List[str], Dict[str,int], int]:\n",
    "    in_path = Path(in_dir)\n",
    "    files = sorted(in_path.glob(\"*.jsonl\"))\n",
    "    seen_keys = set()\n",
    "    kept: List[str] = []\n",
    "    stats = {\n",
    "        \"files_total\": len(files),\n",
    "        \"lines_raw\": 0,\n",
    "        \"kept\": 0,\n",
    "        \"dupe\": 0,\n",
    "        \"len_drop\": 0,\n",
    "        \"charset_drop\": 0,\n",
    "        \"json_errors\": 0,  # (approx; we skip but don't count per-line errors distinctly)\n",
    "    }\n",
    "\n",
    "    with tqdm(total=len(files), desc=\"Files\", unit=\"file\") as pbar:\n",
    "        for fp in files:\n",
    "            try:\n",
    "                for raw_claim in iter_claims_from_file(fp, ALL_CLAIMS):\n",
    "                    stats[\"lines_raw\"] += 1\n",
    "                    t = clean_claim(raw_claim, REFS_MODE, ADD_EOS, NFKC)\n",
    "\n",
    "                    # Length guards\n",
    "                    if (MIN_LEN and len(t) < MIN_LEN) or (MAX_LEN and len(t) > MAX_LEN):\n",
    "                        stats[\"len_drop\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    # Charset guards\n",
    "                    if not is_line_charset_ok(t):\n",
    "                        stats[\"charset_drop\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    # Dedup (normalized key)\n",
    "                    if DEDUPE:\n",
    "                        key = dedupe_key(t)\n",
    "                        if key in seen_keys:\n",
    "                            stats[\"dupe\"] += 1\n",
    "                            continue\n",
    "                        seen_keys.add(key)\n",
    "\n",
    "                    kept.append(t)\n",
    "                    stats[\"kept\"] += 1\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "\n",
    "    return kept, stats, len(files)\n",
    "\n",
    "def write_splits(lines: List[str], out_prefix: str,\n",
    "                 train_r: float, val_r: float, test_r: float,\n",
    "                 seed: int = 42) -> Dict[str,int]:\n",
    "    random.Random(seed).shuffle(lines)\n",
    "    n = len(lines)\n",
    "    n_train = int(round(train_r * n))\n",
    "    n_val   = int(round(val_r   * n))\n",
    "    n_test  = n - n_train - n_val  # remainder to test to ensure sum == n\n",
    "\n",
    "    train = lines[:n_train]\n",
    "    val   = lines[n_train:n_train+n_val]\n",
    "    test  = lines[n_train+n_val:]\n",
    "\n",
    "    out_train = f\"{out_prefix}_train.txt\"\n",
    "    out_val   = f\"{out_prefix}_val.txt\"\n",
    "    out_test  = f\"{out_prefix}_test.txt\"\n",
    "\n",
    "    for path, arr in [(out_train, train), (out_val, val), (out_test, test)]:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for t in arr:\n",
    "                f.write(t + \"\\n\")\n",
    "\n",
    "    return {\"train\": len(train), \"val\": len(val), \"test\": len(test),\n",
    "            \"out_train\": out_train, \"out_val\": out_val, \"out_test\": out_test}\n",
    "\n",
    "# ---------- Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    tr_p, va_p, te_p = validate_splits(TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n",
    "    print(f\"Split ratios → train {tr_p}%, val {va_p}%, test {te_p}%\")\n",
    "\n",
    "    lines, stats, nfiles = build_corpus(INPUT_FOLDER)\n",
    "    print(\"\\n=== Stats ===\")\n",
    "    print(f\"Files processed           : {nfiles}\")\n",
    "    print(f\"Raw claims seen           : {stats['lines_raw']}\")\n",
    "    print(f\"Kept (after cleaning/dedupe): {stats['kept']}\")\n",
    "    print(f\"Dropped (length)          : {stats['len_drop']}\")\n",
    "    print(f\"Dropped (charset)         : {stats['charset_drop']}\")\n",
    "    print(f\"Duplicates removed        : {stats['dupe']}\")\n",
    "\n",
    "    split_info = write_splits(lines, OUTPUT_PREFIX,\n",
    "                              TRAIN_RATIO, VAL_RATIO, TEST_RATIO,\n",
    "                              seed=RANDOM_SEED)\n",
    "    print(\"\\n=== Outputs ===\")\n",
    "    print(f\"Train lines: {split_info['train']} → {split_info['out_train']}\")\n",
    "    print(f\"Val lines  : {split_info['val']}   → {split_info['out_val']}\")\n",
    "    print(f\"Test lines : {split_info['test']}  → {split_info['out_test']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac0be3",
   "metadata": {},
   "source": [
    "# All claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97698c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split ratios → train 98%, val 1%, test 1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f22e5acef241b28a50fdc8d137b79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/204 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stats ===\n",
      "Files processed           : 204\n",
      "Raw claims seen           : 4593858\n",
      "Kept (after cleaning/dedupe): 4420734\n",
      "Dropped (length)          : 82477\n",
      "Dropped (charset)         : 0\n",
      "Duplicates removed        : 90647\n",
      "\n",
      "=== Outputs ===\n",
      "Train lines: 4332319 → ../data/corpus_train.txt\n",
      "Val lines  : 44207   → ../data/corpus_val.txt\n",
      "Test lines : 44208  → ../data/corpus_test.txt\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# patnroberta - All Claims Corpus Builder\n",
    "# ==============================\n",
    "# - Processes EPO JSONL files containing claims\n",
    "# - Cleans text, normalizes reference numerals, dedupes\n",
    "# - Produces train/val/test line-separated corpora with adjustable ratios\n",
    "# ==============================\n",
    "\n",
    "from __future__ import annotations\n",
    "import re, json, unicodedata, string, random, math\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Iterable, Tuple, Dict, List\n",
    "\n",
    "# ---------- Config ----------\n",
    "INPUT_FOLDER = \"../data/0-scraped\"\n",
    "OUTPUT_PREFIX = \"../data/corpus\"   # will write *_train.txt, *_val.txt, *_test.txt\n",
    "\n",
    "# Cleaning/normalization\n",
    "REFS_MODE    = \"replace\"     # \"keep\" | \"remove\" | \"replace\"  (replace -> <REFNUM>)\n",
    "ADD_EOS      = False         # RoBERTa does not need EOS in raw text\n",
    "ALL_CLAIMS   = True          # True = keep all claims; False = only independent claim (first)\n",
    "NFKC         = True          # normalize to NFKC\n",
    "DEDUPE       = True          # deduplicate lines (normalized key)\n",
    "MIN_LEN      = 20            # 0 = disable\n",
    "MAX_LEN      = 5000          # 0 = disable\n",
    "\n",
    "# Charset guards (byte-BPE friendly): do not penalize non-ASCII\n",
    "MIN_PRINTABLE_RATIO = 0.98   # drop lines with < this fraction of printable chars\n",
    "MAX_NONASCII_RATIO  = 0.0    # 0 disables the non-ASCII cap\n",
    "\n",
    "# Splits (must sum to 1.0). Examples: (0.98, 0.01, 0.01) or (0.90, 0.05, 0.05)\n",
    "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.98, 0.01, 0.01\n",
    "RANDOM_SEED = 42             # for deterministic shuffling\n",
    "\n",
    "# ---------- Patterns ----------\n",
    "# Reference numerals like (101), [0032], {12, 14}, etc.\n",
    "REF_PARENS = r\"\"\"[\\(\\[\\{]\\s*(?:\\d+[A-Za-z]*[′'″]*)(?:\\s*,\\s*\\d+[A-Za-z]*[′'″]*)*\\s*[\\)\\]\\}]\"\"\"\n",
    "REF_REGEX  = re.compile(REF_PARENS)\n",
    "\n",
    "# whitespace collapse\n",
    "WS = re.compile(r\"\\s+\")\n",
    "\n",
    "# digits collapse (for dedupe key)\n",
    "DIGITS = re.compile(r\"\\d+\")\n",
    "\n",
    "# Allow extra useful Unicode in patents\n",
    "PRINTABLE_SET = set(string.printable) | {\"’\",\"“\",\"”\",\"–\",\"—\",\"·\",\"•\",\"°\",\"µ\",\"²\",\"³\",\"±\",\"≥\",\"≤\",\"·\",\"½\",\"¼\",\"¾\",\"™\",\"®\",\"§\"}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_line_charset_ok(s: str) -> bool:\n",
    "    if not s:\n",
    "        return False\n",
    "    total = len(s)\n",
    "    printable = sum((ch in PRINTABLE_SET) or ch.isprintable() for ch in s)\n",
    "    if total == 0 or printable / total < MIN_PRINTABLE_RATIO:\n",
    "        return False\n",
    "    if MAX_NONASCII_RATIO:\n",
    "        nonascii = sum(ord(ch) > 127 for ch in s)\n",
    "        if (nonascii / total) > MAX_NONASCII_RATIO:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def process_ref_numerals(text: str, mode: str) -> str:\n",
    "    if mode == \"keep\":\n",
    "        return text\n",
    "    if mode == \"remove\":\n",
    "        return REF_REGEX.sub(\" \", text)\n",
    "    if mode == \"replace\":\n",
    "        out = REF_REGEX.sub(\" <REFNUM> \", text)\n",
    "        # collapse repeats of <REFNUM>\n",
    "        return re.sub(r\"(?:\\s*<REFNUM>\\s*){2,}\", \" <REFNUM> \", out)\n",
    "    raise ValueError(\"REFS_MODE must be 'keep', 'remove', or 'replace'\")\n",
    "\n",
    "def clean_claim(text: str, refs_mode: str, add_eos: bool, nfkc: bool) -> str:\n",
    "    t = text.strip().replace(\"\\n\", \" \")\n",
    "    if nfkc:\n",
    "        t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = process_ref_numerals(t, refs_mode)\n",
    "    t = WS.sub(\" \", t).strip()\n",
    "    if add_eos and not t.endswith(\"<EOS>\"):\n",
    "        t = f\"{t} <EOS>\"\n",
    "    return t\n",
    "\n",
    "def dedupe_key(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a normalized key for deduplication while keeping original text cased.\n",
    "    - remove <REFNUM>\n",
    "    - collapse digits to 0\n",
    "    - normalize spaces\n",
    "    - lowercase for key only\n",
    "    \"\"\"\n",
    "    k = t.replace(\"<REFNUM>\", \" \")\n",
    "    k = DIGITS.sub(\"0\", k)\n",
    "    k = \" \".join(k.split()).lower()\n",
    "    return k\n",
    "\n",
    "def validate_splits(train_r: float, val_r: float, test_r: float) -> Tuple[int,int,int]:\n",
    "    s = train_r + val_r + test_r\n",
    "    if abs(s - 1.0) > 1e-8:\n",
    "        raise ValueError(f\"Split ratios must sum to 1.0, got {s}\")\n",
    "    # Return as percentages for logging\n",
    "    return int(train_r*100), int(val_r*100), int(test_r*100)\n",
    "\n",
    "# ---------- Main processing ----------\n",
    "def iter_claims_from_file(fp: Path, all_claims: bool) -> Iterable[str]:\n",
    "    with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        for line in fh:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            claims = data.get(\"c\", {})\n",
    "            if isinstance(claims, dict):\n",
    "                values = list(claims.values())\n",
    "            elif isinstance(claims, list):\n",
    "                values = claims\n",
    "            else:\n",
    "                values = []\n",
    "            for i, claim_text in enumerate(values):\n",
    "                if not isinstance(claim_text, str):\n",
    "                    continue\n",
    "                yield claim_text\n",
    "                if not all_claims:\n",
    "                    break  # only the first (independent) claim\n",
    "\n",
    "def build_corpus(in_dir: str) -> Tuple[List[str], Dict[str,int], int]:\n",
    "    in_path = Path(in_dir)\n",
    "    files = sorted(in_path.glob(\"*.jsonl\"))\n",
    "    seen_keys = set()\n",
    "    kept: List[str] = []\n",
    "    stats = {\n",
    "        \"files_total\": len(files),\n",
    "        \"lines_raw\": 0,\n",
    "        \"kept\": 0,\n",
    "        \"dupe\": 0,\n",
    "        \"len_drop\": 0,\n",
    "        \"charset_drop\": 0,\n",
    "        \"json_errors\": 0,  # (approx; we skip but don't count per-line errors distinctly)\n",
    "    }\n",
    "\n",
    "    with tqdm(total=len(files), desc=\"Files\", unit=\"file\") as pbar:\n",
    "        for fp in files:\n",
    "            try:\n",
    "                for raw_claim in iter_claims_from_file(fp, ALL_CLAIMS):\n",
    "                    stats[\"lines_raw\"] += 1\n",
    "                    t = clean_claim(raw_claim, REFS_MODE, ADD_EOS, NFKC)\n",
    "\n",
    "                    # Length guards\n",
    "                    if (MIN_LEN and len(t) < MIN_LEN) or (MAX_LEN and len(t) > MAX_LEN):\n",
    "                        stats[\"len_drop\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    # Charset guards\n",
    "                    if not is_line_charset_ok(t):\n",
    "                        stats[\"charset_drop\"] += 1\n",
    "                        continue\n",
    "\n",
    "                    # Dedup (normalized key)\n",
    "                    if DEDUPE:\n",
    "                        key = dedupe_key(t)\n",
    "                        if key in seen_keys:\n",
    "                            stats[\"dupe\"] += 1\n",
    "                            continue\n",
    "                        seen_keys.add(key)\n",
    "\n",
    "                    kept.append(t)\n",
    "                    stats[\"kept\"] += 1\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "\n",
    "    return kept, stats, len(files)\n",
    "\n",
    "def write_splits(lines: List[str], out_prefix: str,\n",
    "                 train_r: float, val_r: float, test_r: float,\n",
    "                 seed: int = 42) -> Dict[str,int]:\n",
    "    random.Random(seed).shuffle(lines)\n",
    "    n = len(lines)\n",
    "    n_train = int(round(train_r * n))\n",
    "    n_val   = int(round(val_r   * n))\n",
    "    n_test  = n - n_train - n_val  # remainder to test to ensure sum == n\n",
    "\n",
    "    train = lines[:n_train]\n",
    "    val   = lines[n_train:n_train+n_val]\n",
    "    test  = lines[n_train+n_val:]\n",
    "\n",
    "    out_train = f\"{out_prefix}_train.txt\"\n",
    "    out_val   = f\"{out_prefix}_val.txt\"\n",
    "    out_test  = f\"{out_prefix}_test.txt\"\n",
    "\n",
    "    for path, arr in [(out_train, train), (out_val, val), (out_test, test)]:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for t in arr:\n",
    "                f.write(t + \"\\n\")\n",
    "\n",
    "    return {\"train\": len(train), \"val\": len(val), \"test\": len(test),\n",
    "            \"out_train\": out_train, \"out_val\": out_val, \"out_test\": out_test}\n",
    "\n",
    "# ---------- Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    tr_p, va_p, te_p = validate_splits(TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n",
    "    print(f\"Split ratios → train {tr_p}%, val {va_p}%, test {te_p}%\")\n",
    "\n",
    "    lines, stats, nfiles = build_corpus(INPUT_FOLDER)\n",
    "    print(\"\\n=== Stats ===\")\n",
    "    print(f\"Files processed           : {nfiles}\")\n",
    "    print(f\"Raw claims seen           : {stats['lines_raw']}\")\n",
    "    print(f\"Kept (after cleaning/dedupe): {stats['kept']}\")\n",
    "    print(f\"Dropped (length)          : {stats['len_drop']}\")\n",
    "    print(f\"Dropped (charset)         : {stats['charset_drop']}\")\n",
    "    print(f\"Duplicates removed        : {stats['dupe']}\")\n",
    "\n",
    "    split_info = write_splits(lines, OUTPUT_PREFIX,\n",
    "                              TRAIN_RATIO, VAL_RATIO, TEST_RATIO,\n",
    "                              seed=RANDOM_SEED)\n",
    "    print(\"\\n=== Outputs ===\")\n",
    "    print(f\"Train lines: {split_info['train']} → {split_info['out_train']}\")\n",
    "    print(f\"Val lines  : {split_info['val']}   → {split_info['out_val']}\")\n",
    "    print(f\"Test lines : {split_info['test']}  → {split_info['out_test']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
