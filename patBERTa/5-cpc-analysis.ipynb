{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b08d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 194656, 'validation': 10814, 'test': 10814}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29a9899ac644878a151074c9f98bf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/194656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (296 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae478d0c34e48f39336866fc0ba2b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c6ff3147e24932a3ba7a36bc96f8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats @ MAX_LEN= 128\n",
      "train: {'avg_len_tokens': 126.41225032878513, 'p95_len_tokens': 128, 'pct_truncated': 96.08488821305276}\n",
      "val  : {'avg_len_tokens': 126.44451636767154, 'p95_len_tokens': 128, 'pct_truncated': 96.30109117810247}\n",
      "test : {'avg_len_tokens': 126.34390604771592, 'p95_len_tokens': 128, 'pct_truncated': 95.79249121509154}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a00998777642ac868ea247d22657f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/194656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b58b67fa97c4b00a8a0841539fc2021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68d1937c7ad416fbe3dd08f97e15f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved encoded dataset to ../data/cpc_cls_encoded_vs8000_len128\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Step 2 — Tokenize CPC CSVs and save HF dataset\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import RobertaTokenizerFast\n",
    "import numpy as np\n",
    "\n",
    "# --- config ---\n",
    "CSV_DIR   = Path(\"../data/cpc_cls\")\n",
    "OUT_DIR   = Path(\"../data/cpc_cls_encoded_vs8000_len128\")  # change name if MAX_LEN changes\n",
    "TOKENIZER = \"../artifacts/patroberta-tokenizers/vs8000\"\n",
    "MAX_LEN   = 128   # start with 128; rerun with 256 if truncation is high\n",
    "\n",
    "LABELS = list(\"ABCDEFGH\")  # we filtered to single-section A–H\n",
    "label2id = {l:i for i,l in enumerate(LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "# --- load csvs -> HF datasets ---\n",
    "def load_split(name):\n",
    "    df = pd.read_csv(CSV_DIR / f\"cpc_cls_{name}.csv\")\n",
    "    # keep only A–H just in case\n",
    "    df = df[df[\"label\"].isin(LABELS)].copy()\n",
    "    df[\"labels\"] = df[\"label\"].map(label2id).astype(int)\n",
    "    return Dataset.from_pandas(df[[\"text\",\"labels\"]], preserve_index=False)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": load_split(\"train\"),\n",
    "    \"validation\": load_split(\"val\"),\n",
    "    \"test\": load_split(\"test\"),\n",
    "})\n",
    "print({k: len(v) for k,v in ds.items()})\n",
    "\n",
    "# --- tokenize ---\n",
    "tok = RobertaTokenizerFast.from_pretrained(TOKENIZER)\n",
    "tok.model_max_length = MAX_LEN\n",
    "\n",
    "def enc(batch):\n",
    "    out = tok(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "    # track truncation\n",
    "    lengths = [len(ids) for ids in out[\"input_ids\"]]\n",
    "    trunc = [int(l > MAX_LEN) for l in lengths]  # will always be 0 because truncation=True\n",
    "    out[\"len\"] = lengths\n",
    "    out[\"was_truncated\"] = [\n",
    "        1 if (len(tok(t)[\"input_ids\"]) > MAX_LEN) else 0 for t in batch[\"text\"]\n",
    "    ]\n",
    "    out[\"labels\"] = batch[\"labels\"]\n",
    "    return out\n",
    "\n",
    "enc_ds = ds.map(enc, batched=True, remove_columns=[\"text\"])\n",
    "enc_ds = enc_ds.with_format(\"torch\")\n",
    "\n",
    "# --- truncation stats ---\n",
    "def stats(split):\n",
    "    a = enc_ds[split][\"len\"]\n",
    "    t = enc_ds[split][\"was_truncated\"]\n",
    "    return {\n",
    "        \"avg_len_tokens\": float(np.mean(a)),\n",
    "        \"p95_len_tokens\": int(np.percentile(a,95)),\n",
    "        \"pct_truncated\": float(100*np.mean(t)),\n",
    "    }\n",
    "\n",
    "print(\"Stats @ MAX_LEN=\", MAX_LEN)\n",
    "print(\"train:\", stats(\"train\"))\n",
    "print(\"val  :\", stats(\"validation\"))\n",
    "print(\"test :\", stats(\"test\"))\n",
    "\n",
    "# --- save to disk ---\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "enc_ds.save_to_disk(str(OUT_DIR))\n",
    "print(\"Saved encoded dataset to\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
