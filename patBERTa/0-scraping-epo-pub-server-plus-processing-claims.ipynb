{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2f653f",
   "metadata": {},
   "source": [
    "# Scraping granted patents from EPO publication server and processing\n",
    "\n",
    "Similar to `scraping/scraping-epo-pub-server.ipynb` except I want to process into JSONL instead of saving as XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a411f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"20050812\"\n",
    "END_DATE   = \"20210914\"\n",
    "ENDS_WITH  = \"B1\"                     # granted\n",
    "BASE_URL   = \"https://data.epo.org\"      # kept only for date bookkeeping (no XML written)\n",
    "LOG_DIRECTORY = \"../data-test/logs\"\n",
    "OUTPUT_DIR = \"../data-test/ep-b1-claims\"  # where the XML files are saved\n",
    "MAX_WORKERS = 6\n",
    "\n",
    "# Retry / requests\n",
    "RETRIES = 10\n",
    "DELAY   = 1\n",
    "BACKOFF = 1\n",
    "JITTER  = (1, 3)\n",
    "TIMEOUT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9144f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, json, threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "from retry import retry\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.filter import SoupStrainer\n",
    "from tqdm import tqdm\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf444bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(LOG_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def _append_failed(url):\n",
    "    with open(os.path.join(LOG_DIRECTORY, 'failed-urls.csv'), 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([datetime.now().isoformat(), url])\n",
    "\n",
    "def _read_finished_dates():\n",
    "    path = os.path.join(LOG_DIRECTORY, 'finished.csv')\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    with open(path, 'r') as f:\n",
    "        return {row[0] for row in csv.reader(f) if row}\n",
    "\n",
    "def _append_finished_dates(dates):\n",
    "    path = os.path.join(LOG_DIRECTORY, 'finished.csv')\n",
    "    existing = _read_finished_dates()\n",
    "    new_dates = [d for d in dates if d not in existing]\n",
    "    if not new_dates:\n",
    "        return\n",
    "    with open(path, 'a', newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        for d in new_dates:\n",
    "            w.writerow([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae428cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(tries=RETRIES, delay=DELAY, backoff=BACKOFF, jitter=JITTER)\n",
    "def get_response(link):\n",
    "    try:\n",
    "        r = requests.get(link, timeout=TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "    except (requests.RequestException, requests.HTTPError, requests.ConnectionError, requests.Timeout):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5667a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_links_from_response(response_content):\n",
    "    soup = BeautifulSoup(response_content, 'html.parser', parse_only=SoupStrainer('a'))\n",
    "    return [link.get('href') for link in soup if link and link.get('href')]\n",
    "\n",
    "def extract_links_ending_with(page_url, ends_with):\n",
    "    resp = get_response(page_url)\n",
    "    if resp is None:\n",
    "        return []\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser', parse_only=SoupStrainer('a'))\n",
    "    out = []\n",
    "    for a in soup:\n",
    "        href = a.get('href')\n",
    "        if not href:\n",
    "            continue\n",
    "        # EPO pages usually have \".../<docId>/B1\" links; we want those.\n",
    "        if href.endswith(ends_with):\n",
    "            out.append(href)\n",
    "    return out\n",
    "\n",
    "def get_filtered_links(start_date_str, end_date_str):\n",
    "    finished_dates = _read_finished_dates()\n",
    "    url = BASE_URL + \"/publication-server/rest/v1.2/publication-dates/\"\n",
    "    resp = get_response(url)\n",
    "    if resp is None:\n",
    "        return None\n",
    "\n",
    "    links = extract_all_links_from_response(resp.content)\n",
    "    start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "    end_date   = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "\n",
    "    filtered = []\n",
    "    for href in links:\n",
    "        try:\n",
    "            date_str = href.split('/')[-2]\n",
    "            link_date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            if start_date <= link_date <= end_date and date_str not in finished_dates:\n",
    "                filtered.append(BASE_URL + href)\n",
    "        except (ValueError, IndexError):\n",
    "            pass\n",
    "    return filtered\n",
    "\n",
    "def get_date_from_url(url):\n",
    "    return url.rstrip('/').split('/')[-2]\n",
    "\n",
    "def extract_all_links(date_links):\n",
    "    \"\"\"Return {date: [full_document_xml_urls]}\"\"\"\n",
    "    extracted = {}\n",
    "    with ThreadPoolExecutor(MAX_WORKERS) as ex:\n",
    "        fut2url = {ex.submit(extract_links_ending_with, url, ENDS_WITH): url for url in date_links}\n",
    "        for fut in as_completed(fut2url):\n",
    "            url = fut2url[fut]\n",
    "            try:\n",
    "                links = fut.result() or []\n",
    "            except Exception as exc:\n",
    "                print(f'{url!r} generated an exception: {exc}')\n",
    "                links = []\n",
    "            date = get_date_from_url(url)\n",
    "            extracted.setdefault(date, [])\n",
    "            processed = [BASE_URL + link + \"/document.xml\" for link in links]\n",
    "            extracted[date].extend(processed)\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8aa867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_xml_bytes(xml_bytes):\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "    try:\n",
    "        root = etree.fromstring(xml_bytes, parser=parser)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    country = root.get('country', '') or ''\n",
    "    number  = root.get('doc-number', '') or ''\n",
    "    kind    = root.get('kind', '') or ''\n",
    "    pn = f\"{country}{number}{kind}\".strip()\n",
    "    if not pn:\n",
    "        return None\n",
    "\n",
    "    # Claims (EN only)\n",
    "    claims_dict = {}\n",
    "    for claim in root.xpath('//claims[@lang=\"en\"]//claim'):\n",
    "        num = (claim.get('num') or '').lstrip('0')\n",
    "        if not num:\n",
    "            continue\n",
    "        texts = []\n",
    "        for ctext in claim.xpath('.//claim-text'):\n",
    "            texts.append(\" \".join(s.strip() for s in ctext.xpath('.//text()') if s and s.strip()))\n",
    "        claim_text = \"\\n\".join(t for t in texts if t)\n",
    "        if claim_text:\n",
    "            claims_dict[num] = claim_text.strip()\n",
    "\n",
    "    return {\"pn\": pn, \"c\": (claims_dict or {})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14fbaa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "_file_locks = {}\n",
    "_global_lock = threading.Lock()\n",
    "\n",
    "def _get_file_lock(date):\n",
    "    with _global_lock:\n",
    "        if date not in _file_locks:\n",
    "            _file_locks[date] = threading.Lock()\n",
    "        return _file_locks[date]\n",
    "\n",
    "def process_link_to_jsonl(url, date):\n",
    "    resp = get_response(url)\n",
    "    if resp is None:\n",
    "        _append_failed(url)\n",
    "        return 0\n",
    "    data = extract_json_from_xml_bytes(resp.content)\n",
    "    if data is None:\n",
    "        return 0  # skip non-CPC or malformed\n",
    "    outfile = os.path.join(OUTPUT_DIR, f\"{date}.jsonl\")\n",
    "    lock = _get_file_lock(date)\n",
    "    with lock, open(outfile, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "    return 1\n",
    "\n",
    "def process_all_to_jsonl(extracted_links_dict):\n",
    "    total = sum(len(v) for v in extracted_links_dict.values())\n",
    "    pbar = tqdm(total=total, desc=\"Fetching + parsing (CPC-only)\")\n",
    "    processed = 0  # number of JSONL records written (CPC-only)\n",
    "\n",
    "    with ThreadPoolExecutor(MAX_WORKERS) as ex:\n",
    "        futures = []\n",
    "        for date, links in extracted_links_dict.items():\n",
    "            for link in links:\n",
    "                futures.append(ex.submit(process_link_to_jsonl, link, date))\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                processed += fut.result() or 0\n",
    "            except Exception:\n",
    "                pass\n",
    "            pbar.update()\n",
    "\n",
    "    pbar.close()\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa2fd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cpc_batch(start_date=START_DATE, end_date=END_DATE):\n",
    "    pages = get_filtered_links(start_date, end_date)\n",
    "    if not pages:\n",
    "        print(\"No date pages to fetch.\")\n",
    "        return\n",
    "    links = extract_all_links(pages)\n",
    "    print(f\"Found {sum(len(v) for v in links.values())} document.xml links across {len(links)} dates.\")\n",
    "    process_all_to_jsonl(links)           # skips non-CPC before any file I/O\n",
    "    _append_finished_dates(list(links.keys()))\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1318364 document.xml links across 839 dates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching + parsing (CPC-only):   0%|          | 0/1318364 [00:00<?, ?it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f4d1b952f90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mjh/patent-llms/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Fetching + parsing (CPC-only):   0%|          | 3663/1318364 [03:24<15:47:39, 23.12it/s]"
     ]
    }
   ],
   "source": [
    "run_cpc_batch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
