{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d42b039",
   "metadata": {},
   "source": [
    "# Scraping granted patents from EPO publication server\n",
    "\n",
    "https://data.epo.org/publication-server/rest/v1.2/publication-dates/ - hardcoded to work for this domain.\n",
    "\n",
    "Notebook scrapes granted patents (determined by URL ending with **B1**, e.g. EP3963522NWB1) and saves them in subfolders by week (YYYYMMDD) like they are listed on publication server. Possible to change to A1 and scrape patent applications, for example, or remove function entirely to scrape granted patents and patent applications. Logic behind scraping granted patents is that they are legally valid, and correctly formatted. Serve as better training data.\n",
    "\n",
    "*The scripts overall process is:*\n",
    "- Get all weekly publication pages for given date range\n",
    "- Get **B1** links to granted patent publications for each weekly publication page\n",
    "- Filter links by what has already been downloaded (against .csv files)\n",
    "- Attempt to download remaining files.\n",
    "\n",
    "If script fails at downloading some links, simply clear logs (finished.csv) and run again. Notebook autotmatically detect which files have not been downloaded and tries them again. I.e. failed-urls.csv for reference and monitoring.\n",
    "\n",
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab491e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range to attempt to download\n",
    "START_DATE = \"20240730\"\n",
    "END_DATE = \"20250730\"\n",
    "\n",
    "# Patent kind code to download; B1 = granted patent\n",
    "ENDS_WITH = 'B1'\n",
    "\n",
    "# Directory to save the downloaded files and subdirectory system of /YYYYMMDD/\n",
    "DIRECTORY = \"./data/ep-b1\"\n",
    "\n",
    "#Directory to save the finished.csv and failed-urls.csv. Finished comprises the dates which have been completed and failed urls those that failed to download. \n",
    "LOG_DIRECTORY = \"./data/logs\"\n",
    "\n",
    "# Added to the relative URLs to get get valid full URL.\n",
    "BASE_URL = \"https://data.epo.org\"\n",
    "\n",
    "# Max workers for thread pool executor. Found 4 was a good limit for free VPN.\n",
    "MAX_WORKERS = 6\n",
    "\n",
    "# Retry settings\n",
    "RETRIES = 10\n",
    "DELAY = 1\n",
    "BACKOFF = 1\n",
    "JITTER = (1, 3)\n",
    "TIMEOUT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from retry import retry\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.filter import SoupStrainer\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Handle requests, retries and errors\n",
    "@retry(tries=RETRIES, delay=DELAY, backoff=BACKOFF, jitter=JITTER)\n",
    "def get_response(link):\n",
    "    try:\n",
    "        response = requests.get(link, timeout=TIMEOUT)\n",
    "        response.raise_for_status()  # If the response contains an HTTP error status code, raise an exception\n",
    "        return response\n",
    "    except (requests.exceptions.RequestException, requests.exceptions.HTTPError, requests.exceptions.ConnectionError, requests.exceptions.Timeout) as err:\n",
    "        #print (\"Error:\", err)\n",
    "        return None\n",
    "\n",
    "#Extract all a tags from the response\n",
    "def extract_all_links_from_response(response_content):\n",
    "    # Use SoupStrainer to parse only 'a' tags\n",
    "    soup = BeautifulSoup(response_content, 'html.parser', parse_only=SoupStrainer('a'))\n",
    "\n",
    "    # Extract and return the links\n",
    "    return [link.get('href') for link in soup]\n",
    "\n",
    "  \n",
    "def extract_links_ending_with(page_url, ends_with):\n",
    "  response = get_response(page_url)\n",
    "  if response is None:\n",
    "      return []\n",
    "  \n",
    "  soup = BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a'))\n",
    "  return [link.get('href') for link in soup if link.get('href').endswith(ends_with)]\n",
    "\n",
    "\n",
    "def get_filtered_links(start_date_str, end_date_str):\n",
    "    # Read the finished.csv file into a list of dates to not retrieve links from as all downloaded.\n",
    "    finished_dates = []\n",
    "    try:\n",
    "        with open(LOG_DIRECTORY + '/finished.csv', 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            finished_dates = [row[0] for row in reader]\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, continue with an empty list\n",
    "        pass\n",
    "    \n",
    "    # Define the URL\n",
    "    url = BASE_URL + \"/publication-server/rest/v1.2/publication-dates/\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = get_response(url)\n",
    "    if response is None:\n",
    "      return None\n",
    "    \n",
    "    # Find all 'a' tags (which represent links)\n",
    "    links = extract_all_links_from_response(response.content)\n",
    "    \n",
    "    # Convert date strings to datetime objects\n",
    "    start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "    end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "\n",
    "    # Filter the links based on the dates\n",
    "    filtered_links = []\n",
    "    for href in links:\n",
    "        try:\n",
    "            # Extract the date from the URL path\n",
    "            date_str = href.split('/')[-2]\n",
    "            link_date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            if start_date <= link_date <= end_date and date_str not in finished_dates:\n",
    "                filtered_links.append(BASE_URL + href)\n",
    "        except (ValueError, IndexError):\n",
    "            # Ignore the link if its date cannot be parsed or if it doesn't have a date in the URL path\n",
    "            pass\n",
    "\n",
    "    return filtered_links\n",
    "  \n",
    "def get_date_from_url(url):\n",
    "    # Assuming the date is at the end of the URL after the last slash\n",
    "    date = url.rstrip('/').split('/')[-2]\n",
    "    return date\n",
    "\n",
    "def remove_existing_files(extracted_links_dict):\n",
    "    for date, links in extracted_links_dict.items():\n",
    "        # Create a new list of links that don't exist as files\n",
    "        new_links = [link for link in links if not os.path.isfile(f'{DIRECTORY}/{date}/{link.split(\"/\")[-2]}.xml')]\n",
    "\n",
    "        # Replace the old list with the new one\n",
    "        extracted_links_dict[date] = new_links\n",
    "\n",
    "    return extracted_links_dict\n",
    "\n",
    "def remove_empty_lists_and_write_to_csv(extracted_links_dict):\n",
    "    # Open the CSV file in append mode\n",
    "    os.makedirs(LOG_DIRECTORY, exist_ok=True)\n",
    "    with open(LOG_DIRECTORY + '/finished.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Iterate over a copy of the dictionary keys\n",
    "        for date in list(extracted_links_dict.keys()):\n",
    "            if len(extracted_links_dict[date]) == 0:\n",
    "                # If the list is empty, remove the key from the dictionary\n",
    "                del extracted_links_dict[date]\n",
    "\n",
    "                # Write the date to the CSV file\n",
    "                writer.writerow([date])\n",
    "\n",
    "    return extracted_links_dict\n",
    "\n",
    "def download_file(url, directory, file_name):\n",
    "    # Send a GET request to the URL\n",
    "    response = get_response(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response is not None:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(f\"{DIRECTORY}/{directory}\", exist_ok=True)     \n",
    "        try:\n",
    "            # Write the content to a temporary file\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:  # filter out keep-alive new chunks\n",
    "                        temp_file.write(chunk)\n",
    "             # If the download completed without errors, move the temporary file to the desired location\n",
    "            os.rename(temp_file.name, f'{DIRECTORY}/{directory}/{file_name}')\n",
    "        except Exception as e:\n",
    "            # If an error occurred, delete the temporary file\n",
    "            if os.path.exists(temp_file.name):\n",
    "                os.remove(temp_file.name)\n",
    "            print(f\"Error downloading {file_name} from {url}: {e}\")\n",
    "\n",
    "        # Return the size of the file in bytes\n",
    "        return len(response.content)\n",
    "    else:\n",
    "        with open(LOG_DIRECTORY + '/failed-urls.csv', 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([datetime.now(), url])\n",
    "        return 0\n",
    "\n",
    "def download_all_files(extracted_links_dict):\n",
    "    # Calculate the total number of links\n",
    "    total_links = sum(len(links) for links in extracted_links_dict.values())\n",
    "\n",
    "    # Create a progress bar for the number of links\n",
    "    pbar_links = tqdm(total=total_links, desc=\"Downloading files\")\n",
    "\n",
    "    # Use a ThreadPoolExecutor to download the files concurrently\n",
    "    with ThreadPoolExecutor(MAX_WORKERS) as executor:\n",
    "        # Iterate over the dictionary\n",
    "        for date, links in extracted_links_dict.items():\n",
    "            for link in links:\n",
    "                # Extract the file name from the link\n",
    "                file_name = link.split('/')[-2] + '.xml'\n",
    "\n",
    "                # Submit a new task to the executor to download the file\n",
    "                future = executor.submit(download_file, link, date, file_name)\n",
    "\n",
    "                # Update the progress bars when the task is done\n",
    "                future.add_done_callback(lambda future: pbar_links.update())\n",
    "\n",
    "    # Close the progress bars\n",
    "    pbar_links.close()\n",
    "\n",
    "def extract_all_links(date_links):\n",
    "    extracted_links_dict = {}\n",
    "\n",
    "    with ThreadPoolExecutor(MAX_WORKERS) as executor:\n",
    "        future_to_url = {executor.submit(extract_links_ending_with, url, ENDS_WITH): url for url in date_links}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                links = future.result()\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (url, exc))\n",
    "            else:\n",
    "                # Get the date from the URL\n",
    "                date = get_date_from_url(url)\n",
    "\n",
    "                # Add the links to the dictionary\n",
    "                if date not in extracted_links_dict:\n",
    "                    extracted_links_dict[date] = []\n",
    "                \n",
    "                # Append \"/document.xml\" to each link before adding it to the dictionary\n",
    "                processed_links = [BASE_URL + link + \"/document.xml\" for link in links]\n",
    "                extracted_links_dict[date].extend(processed_links)\n",
    "\n",
    "    return extracted_links_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87008d86",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_links = get_filtered_links(START_DATE, END_DATE)\n",
    "print(f\"No. of dates to scrape: {len(date_links)}\")\n",
    "print(f\"Dates to scrape: {date_links}\")\n",
    "\n",
    "extracted_links_dict = extract_all_links(date_links)\n",
    "\n",
    "print(f\"Total number of links for all dates in given date range: {sum(len(links) for links in extracted_links_dict.values())}\")\n",
    "remove_existing_files(extracted_links_dict)\n",
    "print(f\"Total number of links after removing existing files: {sum(len(links) for links in extracted_links_dict.values())}\")\n",
    "\n",
    "\n",
    "remove_empty_lists_and_write_to_csv(extracted_links_dict)\n",
    "print(f\"Total number of dates after removing finished dates: {len(extracted_links_dict)}\")\n",
    "\n",
    "download_all_files(extracted_links_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
