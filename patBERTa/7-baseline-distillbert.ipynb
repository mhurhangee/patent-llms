{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12735587,"sourceType":"datasetVersion","datasetId":8050182}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# Baseline: DistilRoBERTa fine-tune on CPC (Aâ€“H)\n# ================================================\nfrom pathlib import Path\nimport numpy as np\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (DistilBertTokenizerFast, RobertaTokenizerFast,\n                          AutoTokenizer, AutoModelForSequenceClassification,\n                          Trainer, TrainingArguments)\nfrom sklearn.metrics import accuracy_score, f1_score\nimport pandas as pd\nimport json\n\n# ---- config ----\nCSV_DIR = Path(\"/kaggle/input/cpc-csv\")\nMODEL_NAME = \"distilroberta-base\"\nOUT_METRICS = Path(\"/kaggle/working/baseline_distilroberta-512.json\")\nMAX_LEN = 512\nBATCH_SIZE = 64\nEPOCHS = 3\nLR = 2e-5\n\nLABELS = list(\"ABCDEFGH\")\nlabel2id = {l:i for i,l in enumerate(LABELS)}\nid2label = {i:l for l,i in label2id.items()}\n\n# ---- load CSVs into HF datasets ----\ndef load_split(name):\n    df = pd.read_csv(CSV_DIR / f\"cpc_cls_{name}.csv\")\n    df[\"labels\"] = df[\"label\"].map(label2id)\n    return Dataset.from_pandas(df[[\"text\", \"labels\"]], preserve_index=False)\n\nds = DatasetDict({\n    \"train\": load_split(\"train\"),\n    \"validation\": load_split(\"val\"),\n    \"test\": load_split(\"test\")\n})\n\n# ---- tokenize ----\ntok = AutoTokenizer.from_pretrained(MODEL_NAME)\ndef enc(batch):\n    return tok(batch[\"text\"], truncation=True, max_length=MAX_LEN)\nds = ds.map(enc, batched=True)\n\n# ---- model ----\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=len(LABELS),\n    id2label=id2label,\n    label2id=label2id\n)\n\n# ---- metrics ----\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    labels = p.label_ids\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n        \"weighted_f1\": f1_score(labels, preds, average=\"weighted\")\n    }\n\n# ---- train ----\nargs = TrainingArguments(\n    output_dir=\"/kaggle/working\",\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    num_train_epochs=EPOCHS,\n    eval_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"macro_f1\",\n    greater_is_better=True,\n    report_to=\"none\",\n    fp16=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"validation\"],\n    processing_class=tok,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n# ---- evaluate & save ----\nmetrics = {\n    \"val\": trainer.evaluate(ds[\"validation\"]),\n    \"test\": trainer.evaluate(ds[\"test\"])\n}\n\nOUT_METRICS.parent.mkdir(parents=True, exist_ok=True)\nOUT_METRICS.write_text(json.dumps(metrics, indent=2))\nprint(json.dumps(metrics, indent=2))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-11T16:09:00.479291Z","iopub.execute_input":"2025-08-11T16:09:00.479595Z","iopub.status.idle":"2025-08-11T20:16:05.316714Z","shell.execute_reply.started":"2025-08-11T16:09:00.479571Z","shell.execute_reply":"2025-08-11T20:16:05.316126Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/194656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac9260338ac4153beeb87c9fed69599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10814 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e4e22d6568f4b5f92427eebfb619757"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10814 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a5d34a79a247a9abc9d8b19392f173"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4563' max='4563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4563/4563 4:01:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Macro F1</th>\n      <th>Weighted F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.547400</td>\n      <td>0.360738</td>\n      <td>0.880895</td>\n      <td>0.862315</td>\n      <td>0.881228</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.345700</td>\n      <td>0.321302</td>\n      <td>0.894396</td>\n      <td>0.881132</td>\n      <td>0.894296</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.286700</td>\n      <td>0.315601</td>\n      <td>0.896615</td>\n      <td>0.884094</td>\n      <td>0.896666</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='170' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [85/85 02:51]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{\n  \"val\": {\n    \"eval_loss\": 0.3156011998653412,\n    \"eval_accuracy\": 0.8966154984279637,\n    \"eval_macro_f1\": 0.8840941387207217,\n    \"eval_weighted_f1\": 0.8966655528326992,\n    \"eval_runtime\": 86.2595,\n    \"eval_samples_per_second\": 125.366,\n    \"eval_steps_per_second\": 0.985,\n    \"epoch\": 3.0\n  },\n  \"test\": {\n    \"eval_loss\": 0.3366488218307495,\n    \"eval_accuracy\": 0.8917144442389495,\n    \"eval_macro_f1\": 0.8762084060294124,\n    \"eval_weighted_f1\": 0.891731866323502,\n    \"eval_runtime\": 86.3405,\n    \"eval_samples_per_second\": 125.248,\n    \"eval_steps_per_second\": 0.984,\n    \"epoch\": 3.0\n  }\n}\n","output_type":"stream"}],"execution_count":4}]}