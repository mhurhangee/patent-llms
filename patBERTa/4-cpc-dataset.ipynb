{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfe78b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf83dbede85f4e2dbdf22d3e8b5cb09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/204 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept: 216284  | Discarded: 177958  → {'multi_or_none_sections': 147406, 'length': 30552}\n",
      "Wrote train: 194656 → ../data/cpc_cls/cpc_cls_train.csv\n",
      "Wrote val: 10814 → ../data/cpc_cls/cpc_cls_val.csv\n",
      "Wrote test: 10814 → ../data/cpc_cls/cpc_cls_test.csv\n",
      "train label counts: {'A': 41167, 'B': 34502, 'C': 14684, 'D': 2529, 'E': 7095, 'F': 15157, 'G': 36104, 'H': 43418}\n",
      "val label counts: {'A': 2287, 'B': 1917, 'C': 816, 'D': 140, 'E': 394, 'F': 842, 'G': 2006, 'H': 2412}\n",
      "test label counts: {'A': 2287, 'B': 1917, 'C': 816, 'D': 141, 'E': 394, 'F': 842, 'G': 2005, 'H': 2412}\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Step 1 — Build CPC classification dataset (A–H, Y)\n",
    "# - Input: folder of JSONL files with {\"pn\", \"c\": {...}, \"cpc\": [...]}\n",
    "# - Output: CSVs with two columns: text,label  (Claim 1, single CPC section)\n",
    "# =========================================================\n",
    "from __future__ import annotations\n",
    "import json, re, unicodedata, random, csv\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Dict, Iterable, Tuple, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------- CONFIG (edit) --------\n",
    "INPUT_DIR     = Path(\"../data/ep-b1-claims-cpc\")  # ~204 JSONL files\n",
    "OUT_DIR       = Path(\"../data/cpc_cls\")           # will write train/val/test CSVs\n",
    "SPLIT_RATIOS  = (0.90, 0.05, 0.05)               # train/val/test\n",
    "SEED          = 42\n",
    "\n",
    "# Cleaning options (aligned with tokenizer corpus builder)\n",
    "NFKC          = True\n",
    "MIN_LEN       = 20\n",
    "MAX_LEN       = 5000\n",
    "REPLACE_REFNUM = True   # replace (101), [0032], {12,14} → <REFNUM>\n",
    "\n",
    "# Charset guards (byte-BPE friendly)\n",
    "MIN_PRINTABLE_RATIO = 0.98\n",
    "MAX_NONASCII_RATIO  = 0.0  # 0 = disable\n",
    "\n",
    "# -------- Patterns (same spirit as earlier) --------\n",
    "REF_PARENS = r\"\"\"[\\(\\[\\{]\\s*(?:\\d+[A-Za-z]*[′'″]*)(?:\\s*,\\s*\\d+[A-Za-z]*[′'″]*)*\\s*[\\)\\]\\}]\"\"\"\n",
    "REF_REGEX  = re.compile(REF_PARENS)\n",
    "WS         = re.compile(r\"\\s+\")\n",
    "\n",
    "PRINTABLE_SET = set(\n",
    "    list(map(chr, range(32,127)))\n",
    ") | {\"’\",\"“\",\"”\",\"–\",\"—\",\"·\",\"•\",\"°\",\"µ\",\"²\",\"³\",\"±\",\"≥\",\"≤\",\"½\",\"¼\",\"¾\",\"™\",\"®\",\"§\"}\n",
    "\n",
    "VALID_SECTIONS = set(\"ABCDEFGHY\")  # include Y if present\n",
    "\n",
    "# -------- Helpers --------\n",
    "def is_charset_ok(s: str) -> bool:\n",
    "    if not s: return False\n",
    "    total = len(s)\n",
    "    printable = sum((ch in PRINTABLE_SET) or ch.isprintable() for ch in s)\n",
    "    if printable / total < MIN_PRINTABLE_RATIO:\n",
    "        return False\n",
    "    if MAX_NONASCII_RATIO:\n",
    "        nonascii = sum(ord(ch) > 127 for ch in s)\n",
    "        if (nonascii / total) > MAX_NONASCII_RATIO:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = t.strip().replace(\"\\n\", \" \")\n",
    "    if NFKC:\n",
    "        t = unicodedata.normalize(\"NFKC\", t)\n",
    "    if REPLACE_REFNUM:\n",
    "        t = REF_REGEX.sub(\" <REFNUM> \", t)\n",
    "        t = re.sub(r\"(?:\\s*<REFNUM>\\s*){2,}\", \" <REFNUM> \", t)\n",
    "    t = WS.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def sections_from_cpc(codes: Iterable[str]) -> List[str]:\n",
    "    secs = []\n",
    "    for c in codes or []:\n",
    "        if not isinstance(c, str) or not c: continue\n",
    "        s = c[0].upper()\n",
    "        if s in VALID_SECTIONS:\n",
    "            secs.append(s)\n",
    "    return secs\n",
    "\n",
    "def iter_claim1_and_sections(fp: Path):\n",
    "    with fp.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        for line in fh:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            claims = obj.get(\"c\") or {}\n",
    "            # claim 1 text\n",
    "            c1 = None\n",
    "            if isinstance(claims, dict):\n",
    "                # keys may be \"1\", \"01\", etc.\n",
    "                for k in (\"1\",\"01\"):\n",
    "                    if k in claims and isinstance(claims[k], str):\n",
    "                        c1 = claims[k]; break\n",
    "                if c1 is None and claims:\n",
    "                    # fall back to first item by key order\n",
    "                    k = sorted(claims.keys(), key=lambda x: (len(x), x))[0]\n",
    "                    c1 = claims.get(k) if isinstance(claims.get(k), str) else None\n",
    "            elif isinstance(claims, list) and claims:\n",
    "                c1 = claims[0] if isinstance(claims[0], str) else None\n",
    "            if not c1: \n",
    "                continue\n",
    "\n",
    "            secs = sections_from_cpc(obj.get(\"cpc\"))\n",
    "            yield c1, secs\n",
    "\n",
    "# -------- Build dataset --------\n",
    "files = sorted(INPUT_DIR.glob(\"*.jsonl\"))\n",
    "rows: List[Tuple[str,str]] = []\n",
    "discard_reasons = Counter()\n",
    "\n",
    "for fp in tqdm(files, desc=\"Files\", unit=\"file\"):\n",
    "    for c1, secs in iter_claim1_and_sections(fp):\n",
    "        # reduce to unique section letters\n",
    "        uniq = sorted(set(secs))\n",
    "        if len(uniq) != 1:\n",
    "            discard_reasons[\"multi_or_none_sections\"] += 1\n",
    "            continue\n",
    "        label = uniq[0]\n",
    "        text = clean_text(c1)\n",
    "\n",
    "        if (MIN_LEN and len(text) < MIN_LEN) or (MAX_LEN and len(text) > MAX_LEN):\n",
    "            discard_reasons[\"length\"] += 1\n",
    "            continue\n",
    "        if not is_charset_ok(text):\n",
    "            discard_reasons[\"charset\"] += 1\n",
    "            continue\n",
    "\n",
    "        rows.append((text, label))\n",
    "\n",
    "print(f\"Kept: {len(rows)}  | Discarded: {sum(discard_reasons.values())}  → {dict(discard_reasons)}\")\n",
    "\n",
    "# -------- Stratified split (by label) --------\n",
    "random.seed(SEED)\n",
    "by_label: Dict[str, List[Tuple[str,str]]] = {}\n",
    "for t,l in rows:\n",
    "    by_label.setdefault(l, []).append((t,l))\n",
    "\n",
    "train_r, val_r, test_r = SPLIT_RATIOS\n",
    "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "for l, items in by_label.items():\n",
    "    random.shuffle(items)\n",
    "    n = len(items)\n",
    "    n_train = int(round(train_r * n))\n",
    "    n_val   = int(round(val_r * n))\n",
    "    train = items[:n_train]\n",
    "    val   = items[n_train:n_train+n_val]\n",
    "    test  = items[n_train+n_val:]\n",
    "    splits[\"train\"].extend(train)\n",
    "    splits[\"val\"].extend(val)\n",
    "    splits[\"test\"].extend(test)\n",
    "\n",
    "# Shuffle within each split\n",
    "for k in splits:\n",
    "    random.shuffle(splits[k])\n",
    "\n",
    "# -------- Write CSVs --------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for split_name, items in splits.items():\n",
    "    out_csv = OUT_DIR / f\"cpc_cls_{split_name}.csv\"\n",
    "    with out_csv.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"text\",\"label\"])\n",
    "        w.writerows(items)\n",
    "    print(f\"Wrote {split_name}: {len(items)} → {out_csv}\")\n",
    "\n",
    "# Label distribution report\n",
    "for k in splits:\n",
    "    c = Counter(lbl for _, lbl in splits[k])\n",
    "    print(f\"{k} label counts:\", dict(sorted(c.items())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
