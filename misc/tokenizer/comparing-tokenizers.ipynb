{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837b7675",
   "metadata": {},
   "source": [
    "# Comparing custom tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a3c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"mhurhangee/patent-ind-claim-en\"\n",
    "SPLITS     = [\"validation\", \"test\"]  # will be combined\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "TOKENIZERS = {\n",
    "    \"bpe-8k\": \"mhurhangee/patent-claims-tokenizer-8000\",\n",
    "    \"bpe-4k\": \"mhurhangee/patent-claims-tokenizer-4000\",\n",
    "    \"bpe-16k\": \"mhurhangee/patent-claims-tokenizer-16000\",\n",
    "    \"tiktoken-gpt2\":  \"tiktoken:gpt2\",  # special prefix for tiktoken\n",
    "}\n",
    "\n",
    "SPECIALS = {\"<EOS>\", \"<IDX>\", \"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abcd4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f176f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis corpus size: 55,862 claims\n"
     ]
    }
   ],
   "source": [
    "ds_parts = [load_dataset(DATASET_ID, split=s) for s in SPLITS]\n",
    "ds = concatenate_datasets(ds_parts)\n",
    "\n",
    "# Get cleaned text list\n",
    "texts = [t.strip() for t in ds[\"text\"] if t and t.strip()]\n",
    "print(f\"Analysis corpus size: {len(texts):,} claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "607e1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokenizer(name, tok):\n",
    "    n = len(texts)\n",
    "    lens_tokens = []\n",
    "    lens_chars  = []\n",
    "    freq = Counter()\n",
    "    special_ids = {}\n",
    "    unk_id = None\n",
    "\n",
    "    if hasattr(tok, \"get_vocab\"):  # HF tokenizer\n",
    "        vocab = tok.get_vocab()\n",
    "        special_ids = {s: tok.convert_tokens_to_ids(s) for s in SPECIALS if s in vocab}\n",
    "        unk_id = tok.unk_token_id\n",
    "\n",
    "        for i in tqdm(range(0, n, BATCH_SIZE), desc=f\"{name} tokenizing\"):\n",
    "            batch = texts[i:i+BATCH_SIZE]\n",
    "            enc = tok(batch, add_special_tokens=False)\n",
    "            ids_lists = enc[\"input_ids\"]\n",
    "            lens_tokens.extend(len(x) for x in ids_lists)\n",
    "            lens_chars.extend(len(s) for s in batch)\n",
    "            for ids in ids_lists:\n",
    "                freq.update(ids)\n",
    "\n",
    "    else:  # tiktoken encoder\n",
    "        enc = tok\n",
    "        for i in tqdm(range(0, n, BATCH_SIZE), desc=f\"{name} tokenizing\"):\n",
    "            batch = texts[i:i+BATCH_SIZE]\n",
    "            ids_lists = [enc.encode(s) for s in batch]\n",
    "            lens_tokens.extend(len(x) for x in ids_lists)\n",
    "            lens_chars.extend(len(s) for s in batch)\n",
    "            for ids in ids_lists:\n",
    "                freq.update(ids)\n",
    "\n",
    "    total_tokens = sum(lens_tokens)\n",
    "    uniq_tokens  = len(freq)\n",
    "    unk_count    = freq.get(unk_id, 0) if unk_id is not None else 0\n",
    "    counts = np.array(list(freq.values()))\n",
    "    rare_1 = int((counts == 1).sum())\n",
    "    rare_5 = int((counts <= 5).sum())\n",
    "\n",
    "    tp = np.array(lens_tokens, dtype=np.int32)\n",
    "    cp = np.array(lens_chars, dtype=np.int32)\n",
    "\n",
    "    metrics = {\n",
    "        \"tokens_total\": total_tokens,\n",
    "        \"tokens_per_claim_avg\": tp.mean(),\n",
    "        \"tokens_per_claim_med\": np.median(tp),\n",
    "        \"tokens_per_claim_p95\": np.percentile(tp, 95),\n",
    "        \"chars_per_token_avg\": cp.sum() / total_tokens,\n",
    "        \"vocab_used\": uniq_tokens,\n",
    "        \"rare_tokens_freq==1\": rare_1,\n",
    "        \"rare_tokens_freq<=5\": rare_5,\n",
    "        \"unk_rate_%\": 100.0 * unk_count / total_tokens if total_tokens else 0.0,\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114663cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba72698682054d5e93ef1c7ef07afba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb60648b7384c7a9c00727acb5344f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/531k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d070ff334ddc4b549cd503da69549c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190aaf8e31cc41f5997822c216e18a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe-8k tokenizing:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae8e7ed645e44259eb5def81dfae20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d311ee282d7c42c1a67d9a9cd0842f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/255k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73560f9a0686400bbcbad476883710fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbe71db597c484e9f1be7eef91106d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe-4k tokenizing:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a422bea6b9d466da3b9603358e2b63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38959a986e224478b15c78471369035e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67382c7671e443aa926e284911b1da01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057d5eeaa695423ab7572f6ecba7b7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe-16k tokenizing:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd737b35a6e843f9a458cc44652d68ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tiktoken-gpt2 tokenizing:   0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_total</th>\n",
       "      <th>tokens_per_claim_avg</th>\n",
       "      <th>tokens_per_claim_med</th>\n",
       "      <th>tokens_per_claim_p95</th>\n",
       "      <th>chars_per_token_avg</th>\n",
       "      <th>vocab_used</th>\n",
       "      <th>rare_tokens_freq==1</th>\n",
       "      <th>rare_tokens_freq&lt;=5</th>\n",
       "      <th>unk_rate_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bpe-8k</th>\n",
       "      <td>21188171.0</td>\n",
       "      <td>379.294887</td>\n",
       "      <td>360.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>5.115194</td>\n",
       "      <td>7796.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpe-4k</th>\n",
       "      <td>22783603.0</td>\n",
       "      <td>407.855125</td>\n",
       "      <td>386.0</td>\n",
       "      <td>759.0</td>\n",
       "      <td>4.757000</td>\n",
       "      <td>3879.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpe-16k</th>\n",
       "      <td>20520901.0</td>\n",
       "      <td>367.349916</td>\n",
       "      <td>349.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>5.281523</td>\n",
       "      <td>15483.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiktoken-gpt2</th>\n",
       "      <td>21951768.0</td>\n",
       "      <td>392.964233</td>\n",
       "      <td>373.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>4.937261</td>\n",
       "      <td>25883.0</td>\n",
       "      <td>1374.0</td>\n",
       "      <td>5917.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tokens_total  tokens_per_claim_avg  tokens_per_claim_med  \\\n",
       "bpe-8k           21188171.0            379.294887                 360.0   \n",
       "bpe-4k           22783603.0            407.855125                 386.0   \n",
       "bpe-16k          20520901.0            367.349916                 349.0   \n",
       "tiktoken-gpt2    21951768.0            392.964233                 373.0   \n",
       "\n",
       "               tokens_per_claim_p95  chars_per_token_avg  vocab_used  \\\n",
       "bpe-8k                        706.0             5.115194      7796.0   \n",
       "bpe-4k                        759.0             4.757000      3879.0   \n",
       "bpe-16k                       684.0             5.281523     15483.0   \n",
       "tiktoken-gpt2                 726.0             4.937261     25883.0   \n",
       "\n",
       "               rare_tokens_freq==1  rare_tokens_freq<=5  unk_rate_%  \n",
       "bpe-8k                        21.0                106.0    0.000099  \n",
       "bpe-4k                         9.0                 51.0    0.000092  \n",
       "bpe-16k                       68.0                356.0    0.000102  \n",
       "tiktoken-gpt2               1374.0               5917.0    0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "for name, ident in TOKENIZERS.items():\n",
    "    if ident.startswith(\"tiktoken:\"):\n",
    "        enc_name = ident.split(\":\", 1)[1]\n",
    "        enc = tiktoken.get_encoding(enc_name)\n",
    "        results[name] = analyze_tokenizer(name, enc)\n",
    "    else:\n",
    "        tok = AutoTokenizer.from_pretrained(ident)\n",
    "        results[name] = analyze_tokenizer(name, tok)\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba58da",
   "metadata": {},
   "source": [
    "### Tokenizer Analysis Summary\n",
    "\n",
    "**Sequence Length Efficiency**  \n",
    "- **bpe-16k**: Shortest sequences on average (367 tokens/claim).  \n",
    "- **bpe-8k**: Close behind at 379 tokens/claim.  \n",
    "- **bpe-4k** and **tiktoken-gpt2**: Longer sequences (~408 and ~393 tokens/claim).  \n",
    "\n",
    "**Compression Trade-offs**  \n",
    "- Larger vocabularies reduce token counts: **bpe-16k** > **bpe-8k** > **bpe-4k**.  \n",
    "- `chars_per_token_avg` rises with vocab size — larger merges yield chunkier tokens.  \n",
    "\n",
    "**Vocab Usage**  \n",
    "- **bpe-4k**/**bpe-8k**: Nearly full vocab usage (≥97%).  \n",
    "- **bpe-16k**: ~97% usage (15.5k of 16k tokens).  \n",
    "- **tiktoken-gpt2**: ~52% usage (25.9k of 50k tokens) — lots of unused tokens.  \n",
    "\n",
    "**Rare Token Tail**  \n",
    "- Custom BPEs: Minimal rare tokens (≤356 tokens occur ≤5 times).  \n",
    "- GPT-2: Large rare tail (5,917 tokens ≤5 occurrences) — much vocab wasted on unused patterns.  \n",
    "\n",
    "**UNK Rate**  \n",
    "- All custom BPEs: ~0% unknown tokens — vocab covers domain well.  \n",
    "- GPT-2: 0% by design.  \n",
    "\n",
    "**Conclusion**  \n",
    "Custom BPE tokenizers are far more domain-efficient for patent claims than GPT-2’s general-purpose tokenizer.  \n",
    "They yield shorter sequences, higher vocab utilization, and minimal rare-token waste.  \n",
    "The **16k vocab** gives the best compression, but **8k** is close and may be preferable for smaller LLMs due to a smaller embedding matrix.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
